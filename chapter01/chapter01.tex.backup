\chapter{Asservissement visuel} \label{chap1}

La vision représente pour les humains environ 70\% des données issues des 
perceptions sensorielles externes \cite{purves2004neuroscience} : d'une 
richesse incroyable, c'est aussi, par conséquent, une source d'erreurs 
inépuisable. La vision artificielle ne déroge pas à ses deux principes, d'où 
l'importance du traitement du signal d'une part (dans le but de recueillir et 
interpréter dans cette grande variété d'informations ce qui est pertinent pour 
une tâche prescrite) et d'une stratégie d'asservissement d'autre part (afin de 
stabiliser une tâche malgré les erreurs et incertitudes sur les données).

Nous allons donc dans un premier temps présenter le mode d'acquisition des 
images, puis nous continuerons sur les modèles de perception utilisés. Une fois 
que nous aurons détaillé la manière dont une scène est projetée sur une image, 
nous pourrons dans une seconde section distinguer plusieurs configurations et 
exposer différentes stratégies d'asservissement pouvant être utilisées. Une 
troisi\`eme section sera consacr\'ee \`a la construction des lois de 
commandes, dont nous fournirons quelques exemples. La quatri\`eme section de ce 
chapitre pr\'esentera les sp\'ecificit\'es d'une utilisation de 
l'asservissement visuel dans le contexte des manipulateurs parall\`eles \`a 
c\^ables : apr\`es avoir pr\'esent\'e les principaux travaux ayant \'et\'e 
effectu\'es dans ce domaine, nous pourrons indiquer les angles d'\'etudes que 
nous avons privil\'egi\'es, les probl\'ematiques qui en \'emergent, et les 
pistes de r\'esolution que nous avons emprunt\'ees.
 
\section{Modèles de capteurs et projections} \label{chap1-0}
 
\subsection{De l'oeil à la caméra} \label{chap1-0-0}
 
Sans pour autant chercher à imiter la vision humaine, la vision artificielle 
s'en inspire néanmoins fortement pour ce qui concerne l'acquisition d'une 
image (Fig.\ref{chap01:fig01}). Ainsi, pour une caméra, le diaphragme joue le 
rôle de l'iris et de la 
pupille et détermine la quantité de lumière qui pourra être enregistrée sur un 
intervalle de temps donné. Par la suite, la lentille joue un rôle équivalent à 
la cornée et au cristallin, en faisant converger les rayons lumineux vers la 
rétine sur laquelle sont disséminés environ 125 millions de photorécepteurs : 
c'est au niveau de ces derniers que l'acquisition est véritablement effectuée. 
On distingue parmi les photorécepteurs :
 \begin{figure}[htp]
  \centering
  \includegraphics[width=.95\linewidth]{./chapter01/figures/oeil.png}
    \caption{\footnotesize{Oeil humain vs vision artificielle}}
\label{chap01:fig01}
\end{figure}

 \begin{itemize}
  \item les cônes, généralement impliqués dans la vision diurne. Les cônes 
présen\-tent 3 types de pigments leur permettant de réagir à des longueurs 
d'ondes spécifiques (qui ne recouvrent pas exactement le triplet RGB 
traditionnellement utilisé en traitement d'image)
  \item les bâtonnets, impliqués dans la vision nocturne. Ne possédant qu'un 
seul type de pigments, ils ne peuvent pas discriminer les longueurs d'ondes. En 
revanche, ils sont en moyenne 1000 fois plus sensibles à la lumière que les 
cônes, et leur population correspond à 20 fois celle des cônes.
 \end{itemize}

Afin de simuler l'activité des photorécepteurs de l'oeil humain, les 
dispositifs technologiques les plus récents adoptent des stratégies basées sur 
l'utilisation de filtres placés en amont des capteurs photosensibles, 
permettant 
ainsi une acquisition en séquence (un même récepteur recevra successivement les 
réponses des filtres correspondants aux longueurs d'ondes distinguées) ou 
simultanée (les réponses sont envoyées sur des capteurs photosensibles dédiés).
  
L'information contenue dans les données ainsi recueillies est riche et multiple 
: elle peut-être de nature colorimétrique, géométrique, elle permet de 
ca\-ractériser des déplacements, des déformations. Toutefois, ce qui est vu 
n'est jamais qu'une représentation de ce qui est observé : il est d\`es lors 
fondamental d'exploiter les donn\'ees recueillies de mani\`ere \`a tendre 
vers une repr\'esentation exploitable de la scène initiale projetée sur les 
capteurs.

Une premi\`ere option serait de tenter de reconstruire la sc\`ene le plus 
fid\`element possible. On pourra dans ce cas privil\'egier l'utilisation de 
cam\'eras st\'er\'eos (Fig.\ref{chap01:fig02view0})\cite{brandou2006}, ou 
encore de capteurs RGB-D (Fig.\ref{chap01:fig02view1})\cite{siradjuddin2012}. A 
l'aide de cam\'eras st\'er\'eos, nous pouvons percevoir la sc\`ene selon 
plusieurs perspectives, tout comme avec la vision binoculaire. En croisant les 
donn\'ees obtenues \`a partir des diff\'erents points de vue, il est possible 
d'approcher une repr\'esentation tri-dimensionnelle de la sc\`ene acquise au 
sein de l'image. Quant aux capteurs RGB-D, ils compl\`etent les donn\'ees 
acquises gr\^ace \`a une ou plusieurs cam\'eras avec de dispositifs permettant 
de recueillir une information sur la profondeur. La fusion des donn\'ees 
g\'eom\'etriques et colorim\'etriques obtenues par les cam\'eras classiques et 
de la localisation tridimensionnelle de points permet une reconstruction 3D de 
la sc\'ene observable. En multipliant les points de vue (soit par le mouvement, 
soit en utilisant plusieurs dispositifs), ou en exploitant un mod\`ele connu de 
la sc\`ene observ\'e, on sera en mesure d'obtenir une repr\'esentation fid\`ele 
d'un environnement. Ce sont toutefois des dispositifs on\'ereux, qui peuvent 
sembler inutilement intrusifs selon le contexte d'utilisation, et requ\'erant 
bien souvent une puissance de calcul d\'emesur\'ee par rapport aux informations 
dont on a besoin.

\begin{figure}[!ht]
  \centering
      \subfloat[Jean-Luc Godard exp\'erimentant avec la st\'er\'eographie pour 
tourner {\it Adieu au langage 3D}]{\label{chap01:fig02view0}
\includegraphics[width=.53\linewidth]{./chapter01/figures/stereo_godard.jpg}} 
\hfill
    \subfloat[La Kinect de @Microsoft qui a permis la d\'emocratisation de 
l'utilisation des capteurs RGB-D]{\label{chap01:fig02view1}
    \includegraphics[width=.43\linewidth]{./chapter01/figures/kinect.png}}
    \caption{\footnotesize{Exemples de cam\'eras st\'er\'eo et de capteurs RGB-D.}}
\label{chap01:fig02}
\end{figure}

Une seconde option consiste \`a d\'eterminer les transformations 
g\'eom\'etriques d'ob\-jets dans le plan image g\'en\'er\'ees par une 
modification soit d'une partie de la sc\`ene (objet en mouvement), soit de la 
pose de la cam\'era elle-m\^eme, voire des deux simultan\'ement. Ceci permet 
entre autres de s'abstraire en partie au moins de la repr\'esentation 
tridimensionnelle de la sc\`ene projet\'ee sur l'image, en utilisant des 
descriptions alternatives (domaine fr\'equentiel, g\'eom\'etries alg\'ebriques, 
points d'int'er\^ets, descripteurs s\'emantiques, $cdots$).

Le choix d'un type de capteur n'est donc pas anodin, il d\'epend de 
param\`etres aussi divers que la quantit\'e et la densit\'e des informations, 
les connaissances pr\'ealables que l'on peut avoir d'une sc\`ene ou d'un objet, 
du type d'informations disponibles dans l'images et pertinentes par rapport au 
contexte (sc\`enes homog\`enes ou textur\'ees, mobiles ou statiques, diurnes ou 
nocturnes, $\cdots$).

Nous partirons du principe que nous utilisons une seule cam\'era simple, ce 
choix \'etant justifi\'e dans la derni\`ere section de ce chapitre. L'objectif 
est \`a pr\'esent de d\'efinir le mod\`ele de repr\'esentation que nous avons 
utilis\'e, soit la mani\`ere dont la sc\`ene est projet\'ee dans l'image.

\subsection{De la scène à l'image} \label{chap1-0-1}

\subsubsection{Mod\`ele de projection} \label{chap1-0-1-0}

Nous avons privilégié le modèle {\it pinhole} qui offre une approximation 
fiable des caméras perspectives (telles que celles que nous avons utilisées) 
tout en restant formellement simple \cite{Faugeras:1993} \cite{hartley2004}, 
sous les hypothèses du respect des conditions de Gauss (angles de faibles 
incidences), et -- c'est notre cas -- d'une absence de distorsion de la caméra.

Soient ${\bf P} = (X, Y, Z)$ les coordonnées 3-D d'un point dans l'espace et 
${\bf p} = (x_m, y_m)$ ses coordonnées m\'etriques dans l'image. Le modèle de 
projection perspective consiste en une projection centrale de centre $\mathcal 
C$, portée par l'axe ${\bf z}_c$ représentant l'axe optique de la caméra. On 
appelle {\it plan image} le plan se trouvant à distance focale $f$ de la 
caméra, soit $Z = f$. On définit également un point de référence ${\bf c}(x_c,y_c)$ 
dans le plan image comme étant le point d'intersection de l'axe optique et du plan 
image (Fig.\ref{chap01:fig03}).\\

\begin{figure}[h!tp]
  \centering
  \def\svgwidth{.85\linewidth}
  \input{./chapter01/figures/modele_projection.pdf_tex}
    \caption{\footnotesize{Modèle de projection {\it pinhole}}}
\label{chap01:fig03}
\end{figure}

En plus du {\it référentiel-base} $\mathcal R_b$ et du r\'ef\'erentiel de 
l'organe terminal $\mathcal R_e$ (cf. Section \ref{}) s'ajoutent un 
%rajouter reference
{\it référentiel-caméra} noté $\mathcal R_c$, un {\it référentiel-objet} 
$\mathcal R_o$, ainsi qu'un {\it référentiel-image} $\mathcal R_i$. Nous 
noterons également $l_x$ et $l_y$ respectivement la longueur et la hauteur d'un 
pixel dont nous aurons besoin pour la suite.

A partir des coordonnées d'un point ${\bf P}(X, Y, Z)$ exprimées dans le 
référentiel de la caméra, les coordonnées projetées sur le plan images sont 
déduites de la manière suivante :

\begin{equation}
x_m = f \frac{X}{Z}, y_m = f \frac{Y}{Z}
\label{chap01:eq01}
\end{equation}

ou, sous forme matricielle :

\begin{equation}
Z
\begin{bmatrix}
x_m \\y_m \\ 1
\end{bmatrix}
=
\begin{bmatrix}
f & 0 & 0 \\ 0 & f & 0 \\ 0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
X \\ Y \\ Z 
\end{bmatrix}
\label{chap01:eq02}
\end{equation}
soit $\widetilde {\bf p} = {\bf A}{\bf P}$, avec $\widetilde {\bf p} = Z {\bf 
p}$.

Un point dans l'image est généralement représenté par ses coordonnées 
pi\-xelliques (l'origine du référentiel étant supposée localisée en haut à 
gauche de l'image).

La conversion entre les coordonnées métriques $(x_m, y_m)$ et les coordonnées 
pixelliques $(x_p, y_p)$, se fait selon la relation suivante :

\begin{equation}
\left \lbrace
\begin{matrix}
x_p = x_c + x_m/l_x \\
y_p = y_c + y_m/l_y
\end{matrix} \right .
\label{chap01:eq03}
\end{equation}
ce qui donne sous une forme matricielle :

\begin{equation}
\begin{bmatrix}
x_p \\y_p \\ 1
\end{bmatrix}
=
\begin{bmatrix}
\frac 1 {l_x} & 0 & x_c \\ 0 & \frac 1 {l_y} & y_c \\ 0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
x \\ y \\ 1
\end{bmatrix}
\label{chap01:eq04}
\end{equation}
soit ${\bf p}_p = {\bf B} {\bf p}$.

On obtient la relation suivante entre les coordonnées métriques 3-D du point et 
les coordonnées pixelliques dans l'image :
\begin{equation}
Z\begin{bmatrix}
x_p \\y_p \\ 1
\end{bmatrix}
=
\begin{bmatrix}
\alpha_x & 0 & x_c \\ 0 & \alpha_y & y_c \\ 0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
X \\ Y \\ Z
\end{bmatrix}
\label{chap01:eq05}
\end{equation}
avec $\alpha_x = f/l_x$ et $\alpha_y = f/l_y$. Les paramètres $(\alpha_x, 
\alpha_y, x_c, y_c)$ de la matrice ${\bf K} = {\bf A}{\bf B}$ ainsi construite 
sont généralement obtenus par calibration \cite{brown71}, \cite{tsai1986}. La 
matrice ${\bf K}$ nous permet de faire le lien entre les informations 
géométriques disponibles dans l'image et la projection de la scène observée. En 
particulier, nous pouvons à présent définir les coordonnées normalisées ainsi :
\begin{equation}
\left \lbrace
\begin{matrix}
x = X/Z \\
y = Y/Z
\end{matrix} \right .
\label{chap01:eq06}
\end{equation}
indépendantes de la focale, et pouvant être déduites des coordonnées 
pixelliques en utilisant la matrice ${\bf K}^{-1}$.

Enfin, les coordonn\'ees 3D du point \'etant exprim\'ees dans le rep\`ere 
cam\'era, nous voulons pouvoir les traduire dans le r\'ef\'erentiel propre de 
l'objet.

\subsubsection{Changements de rep\`ere} \label{chap1-0-1-1}

Soient \`a pr\'esent ${}^c{\bf P} = ({}^cX, {}^cY, {}^cZ)$ les coordonnées du 
point ${\bf P}$ exprimées dans le référentiel $\mathcal R_c$ et ${}^o{\bf P} = 
({}^oX, {}^oY, {}^oZ)$ ses coordonnées dans le référentiel $\mathcal R_o$. Le 
passage d'un référentiel à l'autre est effectué en utilisant la relation 
${}^c{\bf P} = {}^c{\bf t}_o + {}^c{\bf R}_o {}^o{\bf P}$, avec ${}^c{\bf t}_o$ 
le vecteur $3\times 1$ de translation entre les deux référentiels, ${}^c{\bf 
R}_o$ la matrice $3\times 3$ de rotation correspondant à la rotation autour des 
axes du référentiel. En utilisant les coordonnées homogènes $\widetilde {\bf P}$ 
de ${\bf P}$, il est possible de factoriser cette transformation en une seule 
opération matricielle :
\begin{equation}
{}^c \widetilde {\bf P} = {}^c {\bf M}_o {}^o \widetilde {\bf P}
\label{chap01:eq07}
\end{equation}
soit :
\begin{equation}
\begin{bmatrix}
{}^cX \\ {}^cY \\ {}^cZ \\ 1
\end{bmatrix}
=
\begin{bmatrix}
&  {}^c{\bf R}_o & & {}^c{\bf t}_o \\
& {\bf 0} & & 1
\end{bmatrix}
\begin{bmatrix}
{}^oX \\ {}^oY \\ {}^oZ \\ 1
\end{bmatrix}
\label{chap01:eq08}
\end{equation}

On appelle ${}^c{\bf M}_o$ la {\it matrice homogène de transformation} du 
référentiel $\mathcal R_o$ au référentiel $\mathcal R_c$, dont l'inverse 
s'exprime simplement sous la forme :
\begin{equation}
{}^o{\bf M}_c = 
\begin{bmatrix}
&  {}^c{\bf R}_o^T & & -{}^c{\bf R}_o^T {}^c{\bf t}_o \\
& {\bf 0} & & 1
\end{bmatrix}
\label{chap01:eq09}
\end{equation}

En pratique, il suffit de $6$ param\`etres pour d\'ecrire enti\`erement la 
matrice homog\`ene : $3$ pour la translation et $3$ pour la rotation. Si l'on 
utilise par exemple une repr\'esentation des rotations sous la forme ${\bf r} = 
(\theta {\bf u})$, avec $\bf u$ un vecteur unitaire et $\theta$ l'angle de 
rotation autour de ce vecteur, une matrice homog\`ene ${\bf M}$ peut \^etre 
repr\'esent\'ee par le vecteur suivant :

\begin{equation}
{\bf m} = ({\bf t}, \theta {\bf u})  
\label{chap01:eq10}
\end{equation}

Nous sommes \`a pr\'esent en mesure d'établir une relation entre la scène 
observée dans son référentiel propre, dans le r\'ef\'erentiel de la cam\'era (et 
par extension dans chacun des r\'ef\'erentiels du manipulateur) et sa projection 
dans l'image, exprimée en coordonnées pixelliques, métriques et normalisées. Le 
lecteur remarquera cependant que, s'il est possible directement, connaissant 
${\bf K}$ et ${}^o{\bf M}_c$, de déduire à partir des propriétés de la scène 
les caractéristiques de sa projection, l'opération inverse est dépendante dans 
ce modèle d'une estimation pour chaque point de sa profondeur (coordonnée 
${}^cZ$), ce dont nous aurons \`a tenir compte lorsqu'il s'agira de d\'eterminer 
le type de mesures que nous utiliserons pour d\'eduire les consignes \`a donner 
au manipulateur.

\section{Modalit\'es d'asservissement visuel} 
\label{chap1-1}

Selon la nature du manipulateur et le contexte dans lequel celui-ci est 
utilis\'e, plusieurs strat\'egies d'asservissement visuel peuvent \^etre 
utilis\'ees. Nous distinguons en particulier les modalit\'es architecturales 
(positionnement de la cam\'era par rapport au robot et \`a la sc\`ene) et les 
modalit\'es m\'ethodologiques (espace de r\'egulation).

\subsection{Positionnement de la caméra} \label{chap1-1-0}

Lorsque la cam\'era est plac\'ee en dehors de la partie mobile du manipulateur, 
on parlera de configuration déportée (Fig.\ref{chap01:fig04view0}). Une cam\'era 
d\'eport\'ee peut \^etre fixe ou mobile, et permet g\'en\'eralement 
d'appr\'ehender une sc\`ene d'une mani\`ere globale : elle comporte dans son 
plan image une repr\'esentation de l'organe terminal du robot, et le cas 
\'ech\'eant d'un objet d'int\'er\^et, qui peut tout autant \^etre l'organe 
terminal, que l'une des articulations ou encore un objet cible que l'on 
souhaite manipuler. L'utilisation d'une cam\'era d\'eport\'ee se justifie en 
particulier lorsque l'on souhaite estimer la pose de l'organe terminal par 
rapport \`a un r\'ef\'erentiel fixe. Toutefois, l'utilisation d'une cam\'era 
d\'eport\'ee -- particuli\`erement si elle est fixe -- suppose une sc\`ene 
relativement d\'egag\'ee et un espace de travail compatible avec son champs de 
vision et sa r\'esolution. Il est toujours possible cependant de multiplier les 
dispositifs.

Dans le cas contraire o\`u la cam\'era se trouverait li\'ee \`a l'organe 
terminal d'un manipulateur, on parle en toute logique d'une configuration 
embarquée (Fig.\ref{chap01:fig04view1}). Une cam\'era embarqu\'ee pr\'esente 
l'avantage de pouvoir couvrir l'en\-semble de l'espace de travail accessible au 
manipulateur et de compenser les limites de sa r\'esolution par sa mobilit\'e.
La transformation permettant de passer du r\'ef\'erentiel de l'organe terminal 
au r\'ef\'erentiel de la cam\'era est dans ce cas connue avec une pr\'ecision 
suffisante, et sera g\'en\'eralement fixe.

\begin{figure}[htp]
  \centering
  \subfloat[La caméra est déportée ; la scène comprend l'organe terminal et 
l'objet-cible]{\label{chap01:fig04view0}
    \def\svgwidth{.47\linewidth}
  \input{./chapter01/figures/eye_to_hand.pdf_tex}} \hfill
    \subfloat[La caméra est embarquée ; la scène ne comprend que 
l'objet-cible]{\label{chap01:fig04view1}
    \def\svgwidth{.47\linewidth}
  \input{./chapter01/figures/eye_in_hand.pdf_tex}} 
    \caption{\footnotesize{Positionnements de caméra}}
\label{chap01:fig04}
\end{figure}

L'utilisation d'une cam\'era embarqu\'ee est particuli\`erement adapt\'ee aux 
t\^aches de suivi et de manipulation d'objets. On la pr\'ef\`erera par exemple 
dans le cadre de la robotique mobile, ou encore lorsque l'espace de travail d'un 
syst\`eme robotique ne permet pas l'utilisation de cam\'eras d\'eport\'ees, soit 
en raison de sa nature (robotique mobile a\'erienne, robotique sous-marine par 
exemple), soit de sa taille (robots parall\`eles \`a c\^ables de grande 
envergure).


\subsection{Asservissements en position, basés images et hybrides} 
\label{chap1-1-1}

L'asservissement visuel consiste \`a exploiter les mesures situ\'ees dans une 
image, puis d'en d\'eduire des informations visuelles ${\bf s}$ dont on 
souhaitent qu'elles convergent vers un ensemble de valeurs ${\bf s}^*$ 
correspondant \`a un r\'esultat pr\'e-determin\'e. L'\'ecart entre les valeurs 
mesur\'ees et l'objectif constitue l'erreur ${\bf e}$ de consigne. Cette erreur 
de consigne est alors utilis\'ee pour construire une commande de d\'eplacement 
pour le robot (Fig.\ref{chap01:fig05}). On parle de {\it boucle ouverte} lorsque 
une seule it\'eration est effectu\'ee et suffit \`a produire une commande 
convergeant vers le r\'esultat escompt\'e. On utilise cependant 
g\'en\'eralement un sch\'ema en boucle ferm\'ee consistant \`a déduire 
des mesures effectu\'ees au sein de chaque nouvelle image acquise une 
nouvelle commande, ce qui permettra d'affiner, voire de corriger, les 
trajectoires r\'ealis\'ees par le robot. Un sch\'ema en boucle ferm\'e permet 
ainsi de prendre en compte les diff\'erentes incertitudes li\'ees \`a la 
r\'ealit\'e d'un d\'eplacement du robot par rapport \`a son mouvement 
th\'eorique, et surtout une robustesse aux erreurs de 
mod\`eles : calibration de la cam\'era, estimation de la profondeur, 
incertitude sur la g\'eom\'etrie de l'objet observ\'e.

\begin{figure}[htp]
  \centering
    \def\svgwidth{.95\linewidth}
  \input{./chapter01/figures/bloc_commande.pdf_tex}
    \caption{\footnotesize{Schéma des étapes de l'asservissement visuel}}
\label{chap01:fig05}
\end{figure}

Selon la nature des informations visuelles exploit\'ees, on distingue plusieurs 
types d'asservissement visuel.\\

\noindent {\bf Asservissement 3D :}\\

A partir d'un mod\`ele 3D d'un objet (qui peut-\^etre l'organe terminal) et 
d'une strat\'egie de reconstruction, on cherche \`a estimer la pose courante de 
l'objet par rapport \`a la cam\'era et de la faire converger vers une pose 
d\'esir\'ee, ce que l'on pourra obtenir soit par un mouvement de la cam\'era, 
soit par un d\'eplacement de l'objet. Dans ce cas, ${\bf s}$ correspondra aux 
param\`etres de pose de la cam\'era. On d\'efinit $\mathcal R_c$ et $\mathcal 
R_{c^*}$ comme \'etant respectivement les r\'ef\'erentiels courant et d\'esir\'e 
de la cam\'era, et $\mathcal R_o$ le r\'ef\'erentiel objet. Les 
deux strat\'egies suivantes peuvent par exemple \^etre utilis\'ees 
\cite{samson1991} :
\begin{itemize}
 \item si ${}^c{\bf t}_o$ et ${}^{c^*}{\bf t}_o$ sont les coordonn\'ees de 
l'objet exprim\'ees respectivement dans le r\'ef\'erentiel courant de la 
cam\'era et dans le r\'ef\'erentiel d\'esir\'e, et ${}^{c^o}{\bf R}_c$ la 
matrice de rotation donnant le r\'ef\'erentiel courant par rapport au 
r\'ef\'erentiel d\'esir\'e, alors on a :
 \begin{equation}
 \left \lbrace
 \begin{matrix}
  {\bf s} &=& ({}^c{\bf t}_o,& \theta {\bf u}) \\
  {\bf s}^* &=& ({}^{c^*}{\bf t}_o,& {\bf 0}) \\
  {\bf e} &=& ({}^c{\bf t}_o - {}^{c^*}{\bf t}_o,& \theta {\bf u})
 \end{matrix}
  \right .
\label{chap01:eq11}
\end{equation}
$\theta {\bf u}$ param\'etrisant la matrice de rotation ${}^{c^o}{\bf R}_c$. La 
commande g\'en\'er\'ee \`a partir de cette repr\'esentation de 
l'erreur correspondont \`a une trajectoire en ligne droite dans l'image, mais 
pas pour la cam\'era.
\item au contraire, si on utilise :
\begin{equation}
 \left \lbrace
 \begin{matrix}
  {\bf s} &=& ({}^{c^*}{\bf t}_c,& \theta {\bf u}) \\
  {\bf s}^* &=& (0,& {\bf 0}) \\
  {\bf e} &=& ({}^c{\bf t}_c,& \theta {\bf u})
 \end{matrix}
  \right .
\label{chap01:eq12}
\end{equation}
${}^{c^*}{\bf t}_c$ correspondant ici aux coordonn\'ees de l'origine du 
rep\`ere cam\'era courant par rapport au rep\`ere cam\'era d\'esir\'e, alors 
nous obtenons en th\'eorie une trajectoire en ligne droite pour le d\'eplacement 
de la cam\'era, mais ça ne sera plus le cas pour la trajectoire r\'ealis\'ee 
dans l'image.
\end{itemize}

Ainsi, dans le premier cas, on prend le risque de g\'en\'erer une trajectoire 
pour la cam\'era qui sorte des limites de l'espace de travail, et dans le 
deuxi\`eme cas de voir sortir l'objet d'int\^eret de l'image. L'utilisation de 
l'une ou de l'autre m\'ethode impose donc des v\'erifications suppl\'ementaires 
afin de garantir la r\'ealisation de la t\^ache. Aucune des deux solutions ne 
pr\'esentant un clair avantage sur l'autre, le choix d\'ependra du contexte et 
des strat\'egies algorithmiques mises en place.\\

\noindent {\bf Asservissement 2D :} \\

Au contraire de l'asservissement 3D, l'asservissement 2D que l'on retrouve 
\'egalement sous l'acronyme {\it IBVS} (pour {\it Image-based visual servoing}, 
l'asser\-vissement 3D relevant du {\it Position-based visual servoing}) 
exploite directement les informations visuelles extraites des primitives sans 
passer ni par un mod\`ele de la sc\`ene ou de l'objet, ni par une reconstruction 
quelconque. Lorsque la cam\'era se d\'eplace, cela implique des variations des 
informations visuelles mesurables dans l'image. Si l'on est en capacit\'e 
d'\'etablir une relation entre les variations des informations visuelles et 
les mouvements du capteur, alors il est possible d'obtenir un contr\^oleur qui 
ne repose que sur la r\'egulation des valeurs des primitives. A titre 
d'exemple, sur lequel nous reviendrons dans le prochain chapitre, la mesure de 
l'aire d'une surface et la d\'etermination de son centre de gravit\'e suffisent 
\`a contr\^oler la position d'un manipulateur.

Si un asservissement 2D pr\'esente le clair avantage de proposer une consigne 
d\'etermin\'ee uniquement \`a partir de l'espace du capteur, la construction 
m\^eme de la commande d\'epend souvent d'estimations suppl\'ementaires telles 
que la profondeur en chaque point. La qualit\'e de l'estimation affectera d\`es 
lors celle de la commande. Une solution sera par exemple de recourir \`a 
des capteurs suppl\'ementaires pour obtenir ces estimations. 

De mani\`ere g\'en\'erale, le choix d'une primitive visuelle pourra \^etre 
d\'etermin\'e \`a partir de quatre crit\`eres :
\begin{itemize}
 \item sa disponibilit\'e dans l'image, sa facilit\'e d'extraction et sa 
robustesse aux variations de l'environnement (luminosit\'e, occultation, 
$\cdots$)
  \item son d\'ecouplage par rapport aux degr\'es de libert\'e contr\^ol\'es : 
on souhaite en effet qu'une primitive puisse avoir une r\'eponse forte \`a la 
variation d'un des degr\'es de libert\'e, mais faible aux variations des autres.
  \item son ind\'ependance aux estimations suppl\'ementaires telles que la 
profondeur ou l'orientation du plan de l'image
  \item la convergence du sch\'ema de contr\^ole qui en sera d\'eduit.
\end{itemize}

Le choix d'un ensemble de primitives visuelles prendra donc en compte la 
mani\`ere dont celles-ci se compl\`etent, dans un \'equilibre entre 
d\'ecouplage (pour la robustesse aux erreurs de commande) et redondance (pour 
la compensation des erreurs entre primitives) qui d\'ependra du contexte des 
prises de vue (environnement contr\^ol\'e, dynamique, bruit\'e, $\cdots$).\\

\noindent {\bf Asservissement 2D 1/2 :}\\

Propos\'e par \cite{malis1999}, l'asservissement 2D 1/2 (\'egalement appel\'e 
hybride) combine une approche 2D pour les param\`etres de translation et 3D 
pour les rotations. Pour cela, on utilisera par exemple des points 
d'int\'er\^ets dans l'image, \`a partir desquels on pourra estimer une 
homographie entre l'image courante et une image de r\'ef\'erence. De cette 
homographie seront d\'eduites des mesures permettant de construire ${\bf s}$ \` 
a partir d'informations 2D et 3D, puis de d\'eterminer une consigne -- que 
l'on qualifiera d'hybride -- pour la cam\'era, et enfin une commande pour le 
robot. L'utilisation de l'asservissement 2D 1/2 pr\'esente en particulier 
l'avantage d'offrir un d\'ecouplage des contr\^oles en rotation et en 
translation, mais \'egalement de se dispenser d'une estimation de la profondeur, 
qui sera plut\^ot d\'eduite des homographies successives.


\subsection{Justification des choix de configuration dans le cadre de notre 
\'etude} \label{chap1-1-2}

Nous avons donc le choix des types de capteurs \`a utiliser (cam\'eras 
monoculaires, cam\'eras st\'er\'eos, capteurs RGB-D, $\cdots$), de leur 
disposition, de leur nombre ainsi que de la m\'ethodologie d\'exploitation des 
donn\'ees de l'image et des connaissances {\it a priori} d'une sc\`ene. \\

\noindent {\bf Type de capteurs :}\\

La possibilit\'e d'une reconstruction fid\`ele de la sc\`ene n'en implique 
aucunement la n\'ecessit\'e. En particulier, le robot {\tt Marionet-Assist} sur 
lequel nous avons conduits nos exp\'erimentations a \'et\'e con\c cu dans un 
cadre de robotique de service et d'assistance, d\'eployable donc dans des 
environnements priv\'es (domicile de l'utilisateur ou chambre d'h\^opital par 
exemple). A ce titre, se pose la question de l'intimit\'e et du respect de la 
vie priv\'ee des utilisateurs. Si nous pouvons r\'ealiser avec une cam\'era 
monoculaire les mesures n\'ecessaires \`a la r\'ealisation d'une t\^ache sans 
efforts suppl\'ementaires d\'emesur\'es, ce choix doit \^etre privil\'egi\'e. 
La question n'est pas tant ici de faire mieux avec plus, mais de faire autant 
avec moins, en utilisant les propri\'et\'es des robots \`a c\^ables pour 
obtenir des lois de commande simplifi\'ees et n\'eanmoins pr\'ecises et 
robustes.\\

\noindent {\bf Disposition et nombre des capteurs :}\\

Les robots parall\`eles \`a c\^ables se distinguent par la taille de leur 
espace de travail g\'en\'eralement cons\'equente. A titre d'exemple, le robot 
{\tt Marionet-As\-sist} évolue dans un cube de $4m \times 3m \times 3m$, et le 
volume atteignable par un robot de type {\tt Marionet-Crane} peut aller jusqu'à 
$100m \times 35m \times 35m$. D\`es lors, l'utilisation d'une seule cam\'era 
d\'eport\'ee sera probablement insuffisante pour couvrir l'ensemble de l'espace 
atteignable par le manipulateur. Les solutions envisageables sont donc 
l'utilisation de plusieurs dispositifs d\'eport\'es, d'un dispositif 
embarqu\'e, ou d'un dispositif embarqu\'e compl\'et\'e par un ou plusieurs 
dispositifs d\'eport\'es. Puisque les robots {\tt Marionet} ont \'egalement 
vocation \`a \^etre utilis\'es dans des environnements dynamiques et/ou inconnus 
(pi\`eces de vie, catastrophe naturelle, $\cdots$), il nous semble opportun 
d'assurer la pr\'esence d'au moins un capteur embarqu\'e, afin de pouvoir 
contourner d'eventuels obstacles, ou de pouvoir s'approcher suffisamment pr\`es 
d'une sc\`ene pour compenser d'\'eventuelles limites de r\'esolution. La 
question se pose alors de savoir si une seule cam\'era embarqu\'ee suffit, ou 
s'il est n\'ecessaire de compl\'eter le dispositif avec d'autres capteurs 
d\'eport\'es.

Pour assurer la saisie et la manipulation d'un objet, nous montrerons au 
chapitre suivant qu'une cam\'era embarqu\'ee suffit \`a la convergence de la 
t\^ache d'asservissement. Toutefois, nous avons vu que la jacobienne du robot, 
qui peut \^etre utilis\'ee soit pour un contr\^ole cin\'ematique, soit pour 
v\'erifier l'\'equilibre statique, \'etait d\'ependante de la pose et des 
coordonn\'ees articulaires. Une cam\'era d\'eport\'ee peut donc s'av\'erer 
judicieuse pour servir de capteur ext\'eroceptif. Nous verrons dans la 
derni\`ere section de ce chapitre que cette piste a \'et\'e \'etudi\'ee et 
fournit des r\'esultats tout-\`a-fait int\'eressants.

Toutefois, nous montrerons que notre sch\'ema d'asservissement est robuste aux 
erreurs de mod\'elisations, et donc qu'une estimation pr\'ecise de la pose ou 
des longueurs de c\^ables n'est pas n\'ecessaire. Dans une version simplifi\'ee 
de la commande utilis\'ee pour l'asservissement, nous verrons d'ailleurs que 
nous pouvons nous passer en partie des param\`etres de pose et des 
coordonn\'ees articulaires.

Nous prenons donc le parti dans ce travail de ne travailler qu'avec une seule 
cam\'era embarqu\'ee.\\

\noindent {\bf Strat\'egie d'asservissement :}\\

Une cam\'era embarqu\'ee suffit pour pouvoir utiliser une strat\'egie PBVS en 
multipliant les points de vue, et est totalement adapt\'ee aux asservissements 
2D et hybrides. Pour autant, lorsque nous \'evoquerons les travaux existants 
sur l'asservissement des robots parall\`eles \`a c\^ables, nous verrons que 
l'asservissement 3D a d\'ej`a \'et\'e explor\'e et a donn\'e des r\'esultats 
tout-\`a-fait convaincants. D\`es lors, nous nous concentrerons ici sur les 
asservissements 2D afin de compl\'eter l'\'etat de l'art, l'extension aux 
asservissements hybrides ne pr\'esentant pas de difficult\'e suppl\'ementaire. 
En particulier, un choix particulier de primitives -- les moments 2D -- nous 
permettra de ne pas nous appuyer sur des mesures suppl\'ementaires, telles 
l'estimation de la profondeur.

\section{D\'etermination du sch\'ema de consigne}\label{chap1-2}

Nous reprenons ici les bases de construction d'une loi de commande 
\cite{samson1991}. Le lecteur les retrouvera développées par exemple dans 
\cite{chaumette2008handbook}. Nous nous contentons ici d'exposer les principes 
généraux d'outils que nous avons utilisés tout au long de nos travaux.

\begin{figure}[htp]
  \centering
    \def\svgwidth{.95\linewidth}
  \input{./chapter01/figures/av_schema.pdf_tex}
  \vspace{-15mm}
    \caption{\footnotesize{A l'instant $t$, le robot se trouve dans une pose ${\bf X}_t$ correspondant aux coordonnées articulaires ${\bf \rho}_t$. A l'aide d'une caméra embarquée, il prend des mesures dans l'image $I$. L'objectif de l'asservissement est de déplacer le robot de manière à ce que les valeurs des mesures correspondent à celles de l'image $I^*$, correspondant à une pose ${\bf X}^*$ et à des coordonnées articulaires ${\bf \rho}^*$. Les mesures effectuées dans l'image $I$ sont notées ${\bf s}(t)$, celles de $I^*$ sont notées ${\bf s}^*$. L'écart entre ces valeurs est noté ${\bf e}(t)$. L'élaboration d'une {\it consigne} de déplacement se faire par l'intermédaire d'une matrice d'interaction ${\bf L}_e$ traduisant les variations de l'erreur en ordre de vitesse pour la caméra. La Jacobienne du robot permet alors de transcrire ce déplacement en commande articulaire. La totalité du processus représente la {\it loi de commande}.}}
\label{chap01:fig05b}
\end{figure}


\subsection{Construction de la loi de commande} \label{chap1-2-0}

Soient ${\bf s}(m(t), a)$ un vecteur comprenant de mesures $m(t)$ réalisées au 
sein de l'image sur des primitives choisies et $a$ la param\'etrisation de 
connaissances {\it a priori}, telles les param\`etres intrins\`eques de la 
cam\'era ou encore des connaissances pr\'ealable sur la sc\`ene ou l'objet 
cibl\'e. Le vecteur ${\bf s}^*$ d\'enote les valeurs désirées des mesures.

On construit alors le vecteur :
\begin{equation}
{\bf e} = {\bf s} - {\bf s}^*
\label{chap01:eq13}
\end{equation}
représentant une erreur correspondant \`a la différence entre les valeurs 
courantes et les valeurs désirées.

Nous souhaitons que l'erreur ${\bf e}$ de mesure décroisse de manière 
exponentielle, soit :
\begin{equation}
\dot {\bf e} = - \lambda {\bf e}
\label{chap01:eq14}
\end{equation}
$\lambda$ étant un {\it gain}, permettant de régler la vitesse de convergence.

On choisit ici d'utiliser un contr\^ole en vitesse. Soit le torseur cinématique 
$\bv_c = (\bnu_c, \bomega_c)$, avec $\bnu_c$ et $\bomega_c$ les vitesses 
instantanées respectivement linéaires et angulaires exprim\'ees dans le 
r\'ef\'erentiel-cam'era. Il est nécessaire dans un premier temps de déterminer 
les variations $\dot {\bf s}$ des mesures pour un déplacement donné $\bv_c$ de 
la caméra. Cette relation peut prendre la forme algébrique d'une matrice ${\bf 
L}_s \in \mathcal M_{k\times 6}$ que l'on appelle {\it matrice d'interaction} 
\cite{espiau1992}, $k$ \'etant le nombres de mesures effectuées. Nous avons 
alors la relation :
\begin{equation}
\dot {\bf s} = {\bf L}_s \bv_c
\label{chap01:eq15}
\end{equation}

En combinant (Eq.\ref{chap01:eq13}) et (Eq.\ref{chap01:eq15}), la relation 
entre la variation de l'erreur et la vitesse de la cam\'era se d\'eduit 
imm\'ediatement :
\begin{equation}
\dot {\bf e} = {\bf L}_e \bv_c
\label{chap01:eq16}
\end{equation}
avec ${\bf L}_e = {\bf L}_s$.

D\`es lors, (Eq.\ref{chap01:eq14}) et (Eq.\ref{chap01:eq16}) nous 
donnent :
\begin{equation}
{\bv}_c = - \lambda {\bf L}^+_e {\bf e}
\label{chap01:eq17}
\end{equation}
avec ${\bf L}^+_e$ pouvant \^etre de plusieurs natures :
\begin{itemize}
 \item lorsque $k \geq 6$ et que la matrice ${\bf L}_e$ est de rang plein, 
${\bf L}^+_e$ correspondra \`a la matrice pseudo-inverse de Moore-Penrose de 
${\bf L}_e$, soit ${\bf L}^+_e = ({\bf L}_e^T{\bf L}_e)^{-1}{\bf L}_e^T$.
\item lorsque $k = 6$ et que le d\'eterminant de la matrice d'interaction est 
non-nul, on pourra \'ecrire $\dot {\bv}_c = - \lambda {\bf L}^{-1}_e {\bf e}$.
\end{itemize}

Devant la difficult\'e en pratique de connaître avec exactitude ${\bf 
L}_e$, on utilisera plut\^ot une approximation de la matrice d'interaction, la 
loi de commande finale devenant :
\begin{equation}
{\bf v}_c = - \lambda \widehat {{\bf L}_e^+} {\bf e} 
\label{chap01:eq18}
\end{equation}

Cette estimation d\'epend \`a la fois de la qualit\'e des mesures effectu\'ees 
dans l'image, des hypoth\`eses de mod\'elisation, de l'estimation de 
param\`etres supplementaires, ou encore de la calibration de la cam\'era.

Le choix d'une m\'ethode d'estimation particuli\`ere d\'epend \'evidemment des 
conditions dans lesquelles se d\'eroulent l'asservissement, et affectera 
n\'ecessai\-rement la qualit\'e de celui-ci. Il est usuel d'utiliser ${{\bf 
L}_e^+} = {{\bf L}_{e_*}^+}$ \cite{espiau1992}, soit de prendre pour matrice 
d'interaction celle qui correspond aux valeurs th\'eoriques qui seraient prises 
une fois l'objectif atteint. La matrice est alors constante, mais -- en 
particulier pour de grand d\'eplacements --, il s'agit d'une estimation 
grossi\`ere, notamment lorsqu'un eestimation de la profondeur est requise. Afin 
de trouver un \`equilibre entre qualit\'e des approximations et robustesse aux 
impr\'ecisions, le choix de ${{\bf L}_e^+} = \left ( {{\bf L}_{e}^+} + {{\bf 
L}_{e_*}^+} \right )$ \cite{malis2004} peut se r\'ev\'eler \^etre un bon 
compromis, et offre de meilleures propri\'et\'es de convergence.

La stabilit\'e d'un sch\'ema de consigne peut \^etre \'evalu\'ee gr\^ace au 
crit\`ere de stabilit\'e asymptotique de Lyapunov. On d\'efinit pour cela une 
fonction $\mathcal L = \frac 1 2 ||{\bf }e(t)||^2$, dont la d\'eriv\'ee est 
donn\'ee par :
\begin{equation}
\dot {\mathcal L} = {\bf e}^T {\bf \dot e} 
\label{chap01:eq18b}
\end{equation}
soit :
\begin{equation}
\dot {\mathcal L} = -\lambda {\bf e}^T {\bf L}_e \widehat {{\bf L}^+_e} {\bf e} 
\label{chap01:eq18c}
\end{equation}

Une condition suffisante de stabilit\'e asymptotique au sens de Lyapunov est 
donn\'ee par :
\begin{equation}
{\bf L}_e \widehat {{\bf L}^+_e} > 0
\label{chap01:eq18d}
\end{equation}

Lorsque la matrice ${\bf L}_e$ est carr\'ee (le nombre de mesures 
ind\'ependantes \'etant \'egal au nombre de degr\'es de libert\'e 
contr\^ol\'es, id\'ealement 6) et de rang plein, la stabilit\'e est assur\'ee 
en th\'eorie, puisque l'on aura :
\begin{equation}
{\bf L}_e {{\bf L}^{-1}_e} = \mathbb I_k > 0
\label{chap01:eq18e}
\end{equation}

En pratique, si la matrice $\widehat {{\bf L}^+_e}$ est suffisamment proche de 
${\bf L}^+_e$, la stabilit\'e est assur\'ee.


\subsection{Exemple de matrice d'interaction : le point} \label{chap1-2-1}

Pour rappel, un point dans l'espace ${\bf P} = (X, Y, Z)$ se projette dans 
l'image de la mani\`ere suivante :

\begin{equation}
\left \lbrace
\begin{matrix}
x = X/Z \\
y = Y/Z 
\end{matrix}
\right .
\label{chap01:eq19}
\end{equation}
o\`u $(x,y)$ sont les coordonn\'ees g\'en\'eralis\'ees du point dans l'image.

En d\'erivant cette relation, nous obtenons :
\begin{equation}
\left \lbrace
\begin{matrix}
\dot x = \dot X/Z - X \dot Z/Z^2  = (\dot X - x \dot Z)/Z \\
\dot y = \dot Y/Z - Y \dot Z/Z^2  = (\dot Y - y \dot Z)/Z 
\end{matrix}
\right .
\label{chap01:eq20}
\end{equation}

Soit $\bv = (\bnu, \bomega)$ le torseur cin\'ematique de la cam\'era, la 
relation permettant d'exprimer la vitesse du point ${\bf X}$ par rapport \`a la 
vitesse de la cam\'era s'\'ecrit :

\begin{equation}
\dot {\bf X} = -\bv - \bomega \times {\bf X}
\label{chap01:eq19}
\end{equation}
soit :
\begin{equation}
\left \lbrace
\begin{matrix}
\dot X = -\bnu_x - \bomega_y Z + \bomega_z Y \\
\dot Y = -\bnu_y - \bomega_z X + \bomega_x Z \\
\dot X = -\bnu_z - \bomega_x Y + \bomega_y X \\
\end{matrix}
\right .
\label{chap01:eq21}
\end{equation}

D\`es lors, en combinant (Eq.\ref{chap01:eq18}) et (Eq.\ref{chap01:eq20}), on 
obtient :
\begin{equation}
\left \lbrace
\begin{matrix}
\dot x &=& -\bnu_x/Z +x\bnu_z/Z + xy\bomega_x -(1+x^2)\bomega_y + y \bomega_z \\
\dot y &=& -\bnu_y/Z +y\bnu_z/Z + (1+y^2)\bomega_x -xy\bomega_y - x \bomega_z
\end{matrix}
\right .
\label{chap01:eq22}
\end{equation}

La matrice d'interaction ${\bf L}_x$ reliant la vitesse du point ${\bf x} = 
(x,y)$ au torseur cin\'ematique de la cam\'era est alors :

\begin{equation}
{\bf L}_x =
\begin{bmatrix}
\frac {-1} Z & 0 & \frac x Z & xy & -(1+x^2) & y \\
0 & \frac {-1} Z & \frac y Z & 1+y^2 & -xy & -x
\end{bmatrix}
\label{chap01:eq23}
\end{equation}
soit bien $\dot {\bf x} = {\bf L}_x \bv$.

Afin de contr\^oler $6$ degr\'es de libert\'e, $3$ points au minimum sont 
n\'ecessaires, mais la matrice pourra alors en certaines poses se r\'ev\'eler 
singuli\`ere \cite{michel1993}. Il existe de plus plusieurs minima locaux 
conduisant \`a ${\bf e} = 0$. Pour ces deux raisons, on utilisera $4$ points au 
moins pour contr\^oler le manipulateur.\\

De plus, la matrice \'etant exprim\'ee dans les coordonn\'ees 
g\'en\'eralis\'ees, elle d\'epend des param\`etres de la cam\'era. Elle 
n\'ecessite enfin une approximation de $Z$, et ceci pour chaque point 
permettant de construire la matrice d'interaction. En cons\'equence, la matrice 
th\'eorique que nous venons d'expliciter ne peut \^etre utilis\'ee en l'\'etat, 
nous recourrons donc \`a une approximation, comme vu dans la sous-section 
pr\'ec\'edente (\ref{chap1-2-0}).

\section{Asservissement visuel des robots parallèles à 
câbles}\label{chap1-3}

Nous avons vu que plusieurs caract\'eristiques des robots parall\`elles
\`a c\^ables peuvent alt\'erer la qualit\'e de la manipulation. Ainsi, la
complexit\'e des mod\`eles g\'eom\'etriques et cin\'ematiques par rapport aux
robots s\'eries et aux robots parall\`eles rigides menace d'une part la
pr\'ecision des op\'erations de manipulation lors de la r\'esolution
num\'erique, et d'autre part l'effectuation en temps r\'eel. De plus, la
n\'ecessaire v\'erification de l\'equilibre statique pour toutes les
configurations de c\^ables en tension strictement positive impacte \'egalement
la contrainte de r\'ealisation en temps r\'eel d'une t\^ache ; surtout, il est
tout autant difficile de pr\'evoir quels c\^ables seront en tension lors d'un
d\'eplacement prescrit, que de garantir la constance du caract\`ere positif ou
nul de la tension d'un c\^able sur l'int\'egralit\'e d'un d\'eplacement. Enfin,
les incertitudes m\'ecaniques -- telles par exemple le diam\`etre r\'eel de la
couche d'enroulement d'un c\^able autour des tambours -- ont \'egalement une
influence sur la pr\'ecision du manipulateur.

Il est donc n\'ecessaire, afin de garantir l'efficacit\'e d\'une t\^ache de
manipulation, de simplifier la r\'esolution num\'erique des mod\`eles, de
d\'evelopper une strat\'egie de contr\^ole de la tension dans les diff\'erents
c\^ables, et de corriger enfin la trajectoire du manipulateur lorsque les
incertitudes et erreurs perturbent celle-ci.

L'utilisation de capteurs suppl\'ementaires a \'et\'e propos\'ee \`a de
nombreuses reprises afin de r\'esoudre un ou plusieurs des probl\`emes
\'enonc\'es. Dans le contexte des robots parall\`eles \`a jambes rigides,
plusieurs suggestions ont \'et\'e faites :
\begin{itemize}
 \item mesurer directement la pose de l'organe terminal \`a l'aide de
syst\'emes de lasers et miroirs \cite{marantette1995machine}, \cite{Heeren:1992}
ou d'une cam\'era d\'eport\'ee \cite{dallej.2006}, \cite{paccot.2008} : cela
permet tout autant une simplification des mod\`eles qu'une correction des
erreurs et impr\'ecisions.
\item utiliser des capteurs proprioceptifs afin d'obtenir une repr\'esentation
fid\`ele de l'\'etat des variables articulaires.
\cite{merlet.1993}, \cite{parenti.2000}, ce qui permet \'egalement une
simplification des mod\`eles pouvant aboutir \`a une ex\'ecution en temps
r\'eel. 
\item un contr\^ole r\'ef\'erenc\'e vision bas\'e sur l'exploitation de mesures
des directions des jambes du robot \cite{andreff2007} : dans cette derni\`ere
m\'ethode, une strat\'egie IBVS est utilis\'ee pour d\'efinir un contr\^ole
cin\'ematique. Elle permet de s'affranchir du calcul du MGD, ce qui la rend
{\it a priori} robuste aux erreurs de mod\`eles et de calibrations.
\end{itemize}

En particulier, cette derni\`ere m\'ethode a \'et\'e d\'eclin\'ee r\'ecemment
pour le contr\^ole de robots parall\`eles \`a c\^ables. Elle fait suite \`a
une premi\`ere approche \cite{conf/iros/DallejGAMM11} consistant \`a mettre dans
un premier temps en place un asservissement visuel 3-D (PBVS) sur l'organe
terminal, puis \`a \'elaborer un sch\'ema de contr\^ole dynamique utilisant la
vision pour estimer la pose et la v\'elocit\'e de la plate-forme. Malgr\'e des
r\'esultats prometteurs obtenus en simulation, il reste \`a valider cette
approche sur un prototype r\'eel.

Les m\^emes auteurs ont donc par la suite propos\'e une approche analogue au
suivi de jambes des robots parall\`eles classiques \cite{dallej2012}, en
exploitant cette fois-ci les directions de d\'eparts des c\^ables, compl\'etant
ainsi un syst\`eme de mesures utilisant des capteurs de forces (afin d'estimer
la tension dans les c\^ables), ainsi que plusieurs cam\'eras d\'eport\'ees pour
estimer la pose de la plate-forme. Le sch\'ema d'ensemble utilise donc un 
premier ensemble de quatre cam\'eras filmant un motif sur l'organe terminal pour
proposer un asservissement 3D donnant le torseur cinem\'atique de l'organe
terminal, puis quatre cam\'eras st\'er\'eos donnant la direction tangente aux
d\'eparts de c\^ables, permettant ainsi de simplifier le contr\^ole
cin\'ematique du robot. A nouveau, les r\'esultats pr\'esent\'es en simulation
nous paraissent tout-\`a-fait prometteurs, mais doivent encore \`a notre
connaissance \^etre valid\'es exp\'erimentalement.

Nous avons toutefois choisi de ne pas poursuivre dans la m\^eme voie pour
les trois raisons suivantes :
\begin{itemize}
 \item pour des raisons d'intrusivit\'e et de co\^ut propres aux conditions
d'applica\-tions des prototypes sur lesquels nous avons travaill\'e, le
dispositif de mesure nous a paru trop cons\'equent ; nous avons
de plus privil\'egi\'e des configurations embarqu\'ees pour des 
raisons mentionn\'e pr\'ec\'edemment, \`a savoir le type d'applications 
correspondant aux robots {\tt Marionet} (particuli\`erement les robots {\tt 
Marionet-Assist} et {\tt Marionet-Crane}), mais \'egale\-ment afin de fournir 
un compl\'ement \`a ce que ces auteurs ont d\'evelopp\'e.
\item bien qu'ils fassent durant leurs travaux l'hypoth\`ese de c\^ables
non-\'elastiques, les auteurs supposent une pr\'ecision des mesures de tension
qui nous semble difficilement atteignable pour des robots de cette envergure. Il
ne s'agissait \'evidemment pas du sujet de leur \'etude, mais nous pensons
n\'eanmoins que certaines situations que nous avons rencontr\'ees pourraient
affecter la qualit\'e des r\'esultats annonc\'es. 
\item enfin, le sch\'ema de contr\^ole repose sur l'estimation et l'inversion
de la matrice d'interaction d'une part, et sur l'estimation de la Jacobienne
inverse d'autre part, se pr\'esentant donc sous la forme :
\begin{equation}
\dot {\bf q} = - \lambda \widehat {{\bf J}^{-1}} \widehat{ {\bf L}_e^+} {\bf e} 
\label{intro:eq25}
\end{equation}
Certes, l'estimation des valeurs de la Jacobienne inverse ets 
consid\'erablement simplifi\'ee par l'utilisation de la vision, mais nous 
pensons qu'il est possible d'unifier ces deux \'etapes \`a la condition
d'ex\'ecuter un contr\^ole cin\'ematique \`a l'aide d'une cam\'era embarqu\'ee,
soit une seule matrice :
\begin{equation}
\dot {\bf q} = - \lambda \widehat {\bf K} {\bf e} 
\label{intro:eq26}
\end{equation}
ne n\'ecessitant pas plus de connaissance pr\'ecise de la pose ${\bf X}$ que des
valeurs des variables articulaires $\rho$. Ceci permet en particulier de 
d\'efinir un asservissement exploitant les mesures r\'ealis\'ees dans l'image 
pour obtenir un contr\^ole simplifi\'e exprim\'e directement dans l'espace 
articulaire.
\end{itemize}

Bien que nous ne puissions garantir que cette simplification du sch\'ema de
contr\^ole puisse s'appliquer \`a toutes les configurations de robots \`a
c\^ables, elle s'av\`ere efficace dans notre contexte, particuli\`erement pour
les robots N-1. Sa mise en place implique toutefois certaines conditions qui
peuvent se r\'ev\'eler incompatibles avec un ensemble d'applications, ce que
nous exposerons lors du chapitre suivant.

\section{Conclusions}

L'objectif de ce chapitre \'etait de pr\'esenter les principaux \'el\'ements 
d'asservisse\-ment visuel que nous avons utilis\'e dans le cadre de ce travail. 
Nous avons pour cela commenc\'e par expliciter le mod\`ele de projection 
utilis\'e afin de d\'efinir les diff\'erents syst\`emes de coordonn\'ees 
utilis\'ees et leurs relations. Nous avons ensuite expos\'e plusieurs 
strat\'egies possibles d'asservissement visuel pouvant \^etre exploit\'ees dans 
notre contexte, et pr\'esnt\'e les raisons qui nous ont men\'e \`a faire le 
choix d'un dispositif n'utilisant qu'une cam\'era embarqu\'ee avec une 
strat\'egie bas\'ee image. Dans la troisi\`eme section de ce chapitre, nous 
avons d\'etaill\'e la construction d'un sch\'ema de consigne prenant la forme 
${\bv} = -\lambda \widehat {\bf L}^*_e e$, aboutissant \`a une loi de commande 
pour un robot : $\dot {\bf q} = -\lambda \widehat {{\bf J}^{-1}} \widehat {\bf 
L}^*_e e$. Apr\`es en avoir discut\'e les conditions de convergence et de 
stabilit\'e, nous avons donn\'e un exemple de construction d'une matrice 
d'interaction, coeur du sch\'ema de consigne.

Nous avons enfin pr\'esent\'e les travaux nous semblant les plus pertinents 
dans le domaine de l'asservissement visuel des robots parall\`eles \`a 
c\^able\cite{dallej2011}. Toutefois, nous souhaitons nous en distinguer pour des 
raisons qui tiennent tout \`a la fois \`a la lourdeur du dispositif impliqu\'e, 
mais \'egalement aux strat\'egies utilis\'ees (principalement l'approche PBVS 
pour estimer la pose), qui ne nous semblent pas compatible avec les applications 
de nos robots.

Le travail que nous avons effectu\'e, pr\'esent\'e dans les chapitres suivants, 
a pour objectif d'am\'eliorer la qualit\'e de manipulation d'un robot 
parall\`ele \`a c\^ables. Pour cela, nous avons distingu\'e quatre crit\`eres 
qui nous permettront d'\'evaluer notre
contribution, \`a savoir :
\begin{enumerate}
 \item la {\bf stabilit\'e} de la plate-forme lors d'un d\'eplacement
 \item la {\bf pr\'ecision} de la pose finale
 \item la {\bf robustesse} aux impr\'ecisions et erreurs de mod\`ele, aux
erreurs et impr\'ecisions des estimations des param\`etres de contr\^ole
  \item la {\bf simplicit\'e} de la loi de commande
\end{enumerate}

Dans ce but, la suite de nos travaux se d\'ecoupent de la mani\`ere 
suivante :
\begin{itemize}
 \item dans le chapitre suivant, nous pr\'esenterons dans un premier temps les 
principes d'un asservissement visuel de robots \`a c\^ables dans les conditions 
pratiques que nous avons d\'etaill\'ees pr\'ec\'edemment, ainsi que le choix 
de primitives particulier que nous avons op\'er\'e.
\item nous montrerons ensuite quelles am\'eliorations sont ainsi permises 
concernant la pr\'ecision et la robustesse d'une commande de d\'eplacement et de 
manipulation.
\item enfin, nous concluerons ce chapitre en proposant une loi de commande 
simplifi\'ee permettant d'obtenir directement des images une commande 
articulaire. Nous utiliserons pour cela une premi\`ere estimation de la matrice 
$\bf K$ reliant les coordonn\'ees articulaires aux mesures effectu\'ees dans 
l'image, que nous actualiseront gr\`ace \`a la m\'ethode de Broyden permettant 
de converger rapidement vers une trajectoire lin\'eaire.
\item l'efficacit\'e de cette derni\`ere \'etape n'est cependant garantie 
qu'\`a la condition que nous sachions quels sont les c\^ables qui contr\^olent 
effectivement les d\'eplacements de la plate-forme. Nous proposerons alors dans 
un quatri\`eme chapitre une m\'ethode permettant de s\'electionner la 
configuration que nous d\'esirons en fonctions de crit\`eres que nous 
discuterons.
\item simulations, exp\'erimentations et r\'esultats seront propos\'es dans le 
dernier chapitre de cette \'etude.
\end{itemize}

